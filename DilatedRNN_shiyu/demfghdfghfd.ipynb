{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permuted Pixel MNIST Demo \n",
    "Light weighted demo of our DilatedRNN on Pixel MNist with permutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./models\")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from classification_models import drnn_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "# processed data from the Zoneout paper:\n",
    "# https://github.com/teganmaharaj/wtfcptb/raw/master/char_level_penntree.npz\n",
    "data_path = \"./char_level_penntree.npz\"\n",
    "n_steps = 100 #length of input sequence\n",
    "input_dims = 64 # char embedding dimension\n",
    "n_classes = 50 # vocab size\n",
    "\n",
    "# model config\n",
    "cell_type = \"GRU\"\n",
    "assert(cell_type in [\"RNN\", \"LSTM\", \"GRU\"])\n",
    "hidden_structs = [256] * 7\n",
    "dilations = [1, 2, 4, 8, 16, 32, 64]\n",
    "assert(len(hidden_structs) == len(dilations))\n",
    "\n",
    "# learning config\n",
    "batch_size = 64\n",
    "learning_rate = 1.0e-3\n",
    "training_iters = batch_size * 30000\n",
    "testing_step = 10\n",
    "display_step = 10\n",
    "\n",
    "test_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTB_data(object):\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def random_train_batch(self, batch_size, n_steps):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of sequences of length n_steps from training set.\n",
    "        \"\"\"\n",
    "        batch_x = np.zeros((batch_size, n_steps), dtype=np.int32)\n",
    "        batch_y = np.zeros((batch_size,), dtype=np.int32)\n",
    "        train_size = len(self.data_dict['train'])\n",
    "        offsets = np.random.choice(range(train_size-n_steps-1), batch_size)\n",
    "        for idx in range(batch_size):\n",
    "            offset = offsets[idx]\n",
    "            batch_x[idx,:] = self.data_dict['train'][offset:offset+n_steps]\n",
    "            batch_y[idx] = self.data_dict['train'][offset+n_steps]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def get_validation_batches(self, batch_size, n_steps):\n",
    "        \"\"\"\n",
    "        A generator for all validation data in mini batches.\n",
    "        \"\"\"\n",
    "        validation_size = len(self.data_dict['valid'])\n",
    "        batch_x = np.zeros((batch_size, n_steps), dtype=np.int32)\n",
    "        batch_y = np.zeros((batch_size,), dtype=np.int32)\n",
    "        n_batches = (validation_size-n_steps-1) / batch_size\n",
    "        \n",
    "        for batch_id in range(n_batches):\n",
    "            offset_base = batch_id * batch_size\n",
    "            for idx in range(batch_size):\n",
    "                offset = offset_base + idx\n",
    "                batch_x[idx,:] = self.data_dict['valid'][offset:offset+n_steps]\n",
    "                batch_y[idx] = self.data_dict['valid'][offset+n_steps]\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "ptb_data = PTB_data(np.load(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building a dRNN with GRU cells\n",
      "WARNING:tensorflow:From /home/weihan3/tfenv/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Building layer: multi_dRNN_dilation_1, input length: 100, dilation rate: 1, input dim: 64.\n",
      "=====> Input length for sub-RNN: 100\n",
      "Building layer: multi_dRNN_dilation_2, input length: 100, dilation rate: 2, input dim: 256.\n",
      "=====> Input length for sub-RNN: 50\n",
      "Building layer: multi_dRNN_dilation_4, input length: 100, dilation rate: 4, input dim: 256.\n",
      "=====> Input length for sub-RNN: 25\n",
      "Building layer: multi_dRNN_dilation_8, input length: 100, dilation rate: 8, input dim: 256.\n",
      "=====> 4 time points need to be padded. \n",
      "=====> Input length for sub-RNN: 13\n",
      "Building layer: multi_dRNN_dilation_16, input length: 100, dilation rate: 16, input dim: 256.\n",
      "=====> 12 time points need to be padded. \n",
      "=====> Input length for sub-RNN: 7\n",
      "Building layer: multi_dRNN_dilation_32, input length: 100, dilation rate: 32, input dim: 256.\n",
      "=====> 28 time points need to be padded. \n",
      "=====> Input length for sub-RNN: 4\n",
      "Building layer: multi_dRNN_dilation_64, input length: 100, dilation rate: 64, input dim: 256.\n",
      "=====> 28 time points need to be padded. \n",
      "=====> Input length for sub-RNN: 2\n"
     ]
    }
   ],
   "source": [
    "# build computation graph\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.int32, [None, n_steps])\n",
    "y = tf.placeholder(tf.int32, [None,])\n",
    "\n",
    "char_embeddings = tf.get_variable(\"char_embeddings\", [n_classes, input_dims])\n",
    "x_emb = tf.nn.embedding_lookup(char_embeddings, x)\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# build prediction graph\n",
    "print \"==> Building a dRNN with %s cells\" %cell_type\n",
    "pred = drnn_classification(x_emb, hidden_structs, dilations, n_steps, n_classes, input_dims, cell_type)\n",
    "\n",
    "# build loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "bpc_cost = cost / np.log(2.0)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(bpc_cost, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10, Minibatch Loss: 5.847838\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "train_results = []\n",
    "validation_results = []\n",
    "test_results = []\n",
    "\n",
    "while step * batch_size < training_iters:\n",
    "    batch_x, batch_y = ptb_data.random_train_batch(batch_size, n_steps)    \n",
    "\n",
    "    feed_dict = {\n",
    "        x : batch_x,\n",
    "        y : batch_y\n",
    "    }\n",
    "    bpc_cost_, step_,  _ = sess.run([bpc_cost, global_step, optimizer], feed_dict=feed_dict)    \n",
    "    train_results.append((step_, bpc_cost_))    \n",
    "\n",
    "    if (step + 1) % display_step == 0:\n",
    "        print \"Iter \" + str(step + 1) + \", Minibatch Loss: \" + \"{:.6f}\".format(bpc_cost_)\n",
    "             \n",
    "    if (step + 1) % testing_step == 0:\n",
    "        \n",
    "        # validation performance\n",
    "        batch_bpcs = []\n",
    "        for batch_x, batch_y in ptb_data.get_validation_batches(test_batch_size, n_steps):\n",
    "            feed_dict = {\n",
    "                x : batch_x,\n",
    "                y : batch_y\n",
    "            }\n",
    "            bpc_cost_, step_ = sess.run([bpc_cost, global_step], feed_dict=feed_dict) \n",
    "            batch_bpcs.append(bpc_cost_)\n",
    "        validation_bpc = np.mean(batch_bpcs)\n",
    "        print \"========> Validation BPC: \" + \"{:.6f}\".format(validation_bpc) + \" over %d batches\" % len(batch_bpcs)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
